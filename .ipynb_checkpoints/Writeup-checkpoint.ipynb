{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model Comparisons and Analysis\n",
    "\n",
    " _by Alejandro Capecchi and Joon Park_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index:\n",
    "\n",
    "1. KNN -K Nearest Neighbors\n",
    "2. LDA/QDA - Discriminant Analysis\n",
    "3. Log-Reg - Logistic Regression\n",
    "4. RF - Random Forests\n",
    "5. SVM - Support Vector Machines for classification\n",
    "\n",
    "6. Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as KNN, the K Nearest Neighbors model is a highly flexible classification model. It performs classification by computing the distances between a **query** point and its K nearest neighbors in P dimensional space, P being the number of features. In simpler terms, it finds the nearest K points and takes a tally of their classes, then predicting this new query point to be of the majority class. It is also a lazy learner meaning that it does not fit any training data during the training stage. It merely stores it for when predictions are needed. Because of this, it is an extremely fast algorithm for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that KNN is flexible because it's decision boundary is solely determined by the training data. This flexibility is determined by the K parameter, the number of neighbors. The smaller K is, the more wiggly the boundary is and the more it tends to overfit since it only finds 1 neighbor. As K grows, this boundary becomes more rigid and robust to random noise. Thus, to find the best KNN model we iterate through a range of K's and determine which had the most accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, points can be weighted by distance for different results. The ones involved were uniform and inverse distance, the former treating all distances as equal (hence uniform) while the latter penalizes points that are very far away from the query point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using all 10 features, it was found that on average:\n",
    "\n",
    "1. 86.6% Accuracy, Inverse Distance Weights, K=5\n",
    "2. 85.9% Accuracy, Uniform Weights, K=5\n",
    "\n",
    "Using only the 4 numerical features, it was found that on average:\n",
    "\n",
    "1. 71.4% Accuracy, Uniform Weights, K=13) \n",
    "2. 67.7% Accuracy, Inverse Distance Weights, K=2\n",
    "\n",
    "__All models were run with LOOCV.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating all 10 variables had the best results on predicting whether a student should or should not go to college, with inverse distance slightly boosting predictive accuracy. Limiting the feature size to 4 severely hampered the accuracy of the models. So if using KNN, it is best to stick to all 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model is a model made up of multiple different decision trees. Based on the prediction made by the number of trees, the models take a voting system and makes one single prediction. This machine learning model generates a tree like structure based on the the gini index (measure of variance). The higher the feature is on the tree means that there is lower value of gini index. This means that there is less variance and therefore the model is considered more important. \n",
    "\n",
    "The random forest model contains many different parameters which affect the tree model. One of the important features is 'max_features' which determines how many features the model will consider. One could take all features, take 20% of the features or take the square root of the number of features. Depending on the number of features, there needs to be a balance between computation speed and the diversity within each of the trees. Too little balance means trees look similar in structure and vice a versa. There are other parameters such as the max_depth (which controls the depth of the tree) and min_samples leaf which control the size of the trees and help with overfitting.\n",
    "\n",
    "For our college dataset, these were the list of features which produced the accuracy of 86.5% with cv equal to 5 where each parameter determines the shape of the overall trees: \n",
    "   - 'fselection__k': 8, \n",
    "   - 'poly_features__degree': 1, \n",
    "   - 'random_forest__max_depth': 7, \n",
    "   - 'random_forest__min_samples_leaf': 2, \n",
    "   - 'random_forest__min_samples_split': 5\n",
    "\n",
    "Through this model, we were also able to determine which features the model thought were the most important. The most important feature is the student's average grades in determining whether or not the they will go to college. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model is classification method used to calculate the probability of an event (in this case, binary event) of whether that event will happen. This is because the value of the predictions are between 0 and 1. The logitstic regression model estimates each feature's beta coefficients and uses maximum likelihood estimation to reduce errors and optimize the best values from the training data. This, in turn, generates the probabilities and allows the model to generate predictions. \n",
    "\n",
    "After using GridsearchCV with the CV of 5, we were able to find the best parameters for Logistic Regression with the values of:\n",
    "   - 'fselection__k': 7\n",
    "   - 'logistic_regression__C': 100\n",
    "   - 'poly_features__degree': 1\n",
    "\n",
    "As shown, it chose 7 features and resulted in a test accuracy of 84%. It is interesting to note the large C parameter value of 100 as it means that there isn't much penalty applied to the model in training and thus will not regularize the beta parameters discussed earlier. There are other important parameters to tune such as penalty and solver. However, for the purpose of this dataset, there weren't any differences in terms of speed or accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a48d2defe43df24ad2dd3f55e04edf742d5db627a83880d8835b5a09ea0333"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
