{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model Comparisons and Analysis\n",
    "\n",
    " _by Alejandro Capecchi and Joon Park_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index:\n",
    "\n",
    "1. KNN -K Nearest Neighbors\n",
    "2. LDA/QDA - Discriminant Analysis\n",
    "3. Log-Reg - Logistic Regression\n",
    "4. RF - Random Forests\n",
    "5. SVM - Support Vector Machines for classification\n",
    "\n",
    "6. Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as KNN, the K Nearest Neighbors model is a highly flexible classification model. It performs classification by computing the distances between a **query** point and its K nearest neighbors in P dimensional space, P being the number of features. In simpler terms, it finds the nearest K points and takes a tally of their classes, then predicting this new query point to be of the majority class. It is also a lazy learner meaning that it does not fit any training data during the training stage. It merely stores it for when predictions are needed. Because of this, it is an extremely fast algorithm for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that KNN is flexible because it's decision boundary is solely determined by the training data. This flexibility is determined by the K parameter, the number of neighbors. The smaller K is, the more wiggly the boundary is and the more it tends to overfit since it only finds 1 neighbor. As K grows, this boundary becomes more rigid and robust to random noise. Thus, to find the best KNN model we iterate through a range of K's and determine which had the most accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, points can be weighted by distance for different results. The ones involved were uniform and inverse distance, the former treating all distances as equal (hence uniform) while the latter penalizes points that are very far away from the query point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using all 10 features, it was found that on average:\n",
    "\n",
    "1. 86.6% Accuracy, Inverse Distance Weights, K=5\n",
    "2. 85.9% Accuracy, Uniform Weights, K=5\n",
    "\n",
    "Using only the 4 numerical features, it was found that on average:\n",
    "\n",
    "1. 71.4% Accuracy, Uniform Weights, K=13) \n",
    "2. 67.7% Accuracy, Inverse Distance Weights, K=2\n",
    "\n",
    "__All models were run with LOOCV.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating all 10 variables had the best results on predicting whether a student should or should not go to college, with inverse distance slightly boosting predictive accuracy. Limiting the feature size to 4 severely hampered the accuracy of the models. So if using KNN, it is best to stick to all 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a48d2defe43df24ad2dd3f55e04edf742d5db627a83880d8835b5a09ea0333"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
